Title or Home Page (index.md Update)
‚Ä¢	Project Name: Autonomous Warehouse Patrolling Robot
‚Ä¢	Team Number: Team 8
‚Ä¢	Semester and Year: Spring 2025
‚Ä¢	University, Class, Professor: Arizona State University, RAS 598 Dr. Aukes
________________________________________
Team Project Plan
High-Level Concept Introduction
Our project centers around the design and implementation of an Autonomous Warehouse Patrolling Robot utilizing the TurtleBot4 platform, integrated with the Robot Operating System 2 (ROS2) framework. The overarching goal is to develop a scalable, low-cost robotic solution capable of performing real-time patrolling tasks in structured indoor environments such as warehouses or storage facilities.
Research Question
How can we leverage low-cost mobile robotics platforms to achieve reliable autonomous patrolling in structured indoor warehouse environments, with real-time anomaly detection and environmental adaptability?
This research question drives us to explore and implement a range of robotics technologies and concepts, particularly focusing on the integration of multiple sensor modalities (such as LiDAR, IMU, depth camera, and odometry), robust control algorithms, and autonomy pipelines that balance reactive behavior (e.g., obstacle avoidance) with deliberative planning (e.g., patrol route optimization).
By simulating realistic warehouse conditions, our project will investigate the following:
‚Ä¢	Multi-Sensor Fusion: Combining data from various onboard sensors to generate a coherent model of the robot‚Äôs environment and enhance localization, mapping, and situational awareness.
‚Ä¢	Autonomous Navigation & Patrolling: Implementing SLAM (Simultaneous Localization and Mapping) techniques alongside path planning algorithms to enable the robot to patrol pre-defined or dynamically generated routes.
‚Ä¢	Real-Time Anomaly Detection: Using sensory input (such as depth and visual cues) to detect unexpected objects, humans, or hazards in the robot‚Äôs path or assigned patrol zones.
‚Ä¢	Interactive System Monitoring: Developing a custom graphical user interface (GUI) to display live robot status, environment mapping, anomaly alerts, and control interfaces for manual override or remote supervision.
‚Ä¢	Environmental Adaptability: Equipping the robot with the capability to adapt its behavior based on changing conditions such as blocked paths, dynamic obstacles, or signal loss, ensuring robustness in real-world applications.
Our approach aims not only to implement a working prototype of a patrolling robot but also to provide a generalized framework for deploying autonomous agents in structured environments. Ultimately, this project aspires to demonstrate how affordable hardware combined with modular software architecture can address real-world operational needs in industrial settings ‚Äî with potential extensions into areas like inventory monitoring, safety inspection, and collaborative automation.
________________________________________
Conceptual Figure
 
Figure 1: Conceptual Overview of Warehouse Patrolling Robot
‚Ä¢	TurtleBot4 with LiDAR, depth camera, IMU, and ultrasonic sensors.
‚Ä¢	Navigation path mapped using SLAM.
‚Ä¢	GUI mockup showing patrol progress, anomaly flags, and data logs.
________________________________________
Sensor Integration
Sensor integration lies at the core of our Autonomous Warehouse Patrolling Robot‚Äôs functionality, enabling perception, localization, navigation, and real-time decision-making. Our approach involves the strategic fusion of multiple sensor modalities, each contributing complementary data that enhances the robot‚Äôs understanding of its environment and its ability to operate reliably in dynamic warehouse settings.
 

 Sensors and Their Roles
1.	2D LiDAR (Light Detection and Ranging):
o	Acts as the primary sensor for SLAM (Simultaneous Localization and Mapping).
o	Provides high-resolution 2D scans of the surroundings to detect static and dynamic obstacles, and helps generate occupancy grid maps.
o	LiDAR data will feed into both the local costmap for real-time obstacle avoidance and the global planner for long-term path navigation.
2.	Depth Camera:
o	Used for object detection and human presence recognition, particularly in areas where traditional LiDAR may not provide sufficient vertical detail.
o	Supports anomaly detection by flagging unexpected objects in patrol zones.
o	Enables 3D perception, helpful for future extensions such as object classification or semantic mapping.
3.	IMU (Inertial Measurement Unit):
o	Provides continuous feedback on the robot's orientation and angular velocity.
o	When fused with odometry and LiDAR data, improves localization accuracy, especially in motion-heavy scenarios or sensor dropouts.
o	Plays a key role in dead-reckoning when GPS is not available (as is common indoors).
4.	Ultrasonic Sensors:
o	Placed strategically around the robot to provide short-range obstacle detection.
o	Useful for redundant safety layers, especially when navigating tight spaces or around unstructured, cluttered areas where other sensors might have blind spots.
________________________________________
Integration in ROS2
Each sensor will be managed through dedicated ROS2 nodes, ensuring modularity, scalability, and real-time communication via ROS topics. Integration strategy includes:
‚Ä¢	Sensor Nodes: Each sensor runs in its own node or device-specific package (e.g., rplidar_ros2, realsense2_camera, micro_ros).
‚Ä¢	Sensor Fusion: Using packages like robot_localization, we fuse IMU and odometry with LiDAR-based localization to generate a robust pose estimate (/odom ‚Üí /map transform).
‚Ä¢	Filtering: We apply techniques such as moving average filters, EKFs (Extended Kalman Filters), and range clipping to clean and refine incoming data streams.
‚Ä¢	Visualization & Debugging: During development, we utilize RViz2 to visualize:
o	Real-time LiDAR scans and maps
o	Point clouds from the depth camera
o	IMU orientation data
o	Ultrasonic proximity markers
‚Ä¢	Localization: We employ AMCL (Adaptive Monte Carlo Localization) for probabilistic localization based on the occupancy grid map and laser scans.
________________________________________
Development & Demonstration Phases
‚Ä¢	Development Phase:
o	Raw sensor data will be visualized and logged for calibration and validation.
o	Sensor streams will be debugged individually and in fused configurations to ensure alignment and synchronization.
o	Thresholds and tuning parameters will be iteratively optimized for reliable performance.
‚Ä¢	Final Demonstration:
o	The robot will autonomously patrol a defined warehouse-like indoor environment.
o	Real-time sensor data will inform navigation, obstacle avoidance, and anomaly detection logic.
o	The GUI will display:
ÔÇß	Live occupancy map
ÔÇß	Detected objects or humans
ÔÇß	Sensor status and proximity alerts
o	The robot will respond dynamically to unexpected obstacles, showcasing robust multi-sensor awareness and safe patrolling behavior.

________________________________________
Interaction Plan
The system will include:
‚Ä¢	A custom ROS2-based GUI (built using rqt and Python Qt) to display:
o	Robot live location on a warehouse map.
o	Obstacle detection and patrol path overlay.
o	Anomaly alerts (unauthorized personnel/object detection).
o	System logs (distance traveled, patrol time, power stats).
We will implement basic keyboard and voice control overrides for debugging and semi-autonomous intervention.
 
Figure 2: Sample GUI Mockup
[Sketch/Rendering showing live map, alerts panel, control buttons, system logs]
________________________________________
Control and Autonomy: Enabling Intelligent Warehouse Patrolling
The autonomous capabilities of the warehouse patrolling robot are achieved through a carefully structured, layered control system. This hierarchical architecture allows for robust and adaptable operation, enabling the robot to navigate complex warehouse environments, react to unforeseen circumstances, and execute its patrolling mission effectively.
1. Low-Level Control: Precise Motion Execution
At the foundational layer lies the low-level control system. This layer is responsible for the precise execution of motion commands sent by higher-level controllers. It directly interfaces with the robot's actuators, primarily the wheel motors.
‚Ä¢	Wheel Velocity Commands: The primary function of this layer is to translate desired linear and angular velocities into individual wheel velocity commands. This involves kinematic models that account for the robot's geometry and wheel configuration.
‚Ä¢	Odometry Feedback: To ensure accurate motion, the low-level controller relies on odometry feedback. Data from wheel encoders (or other internal sensors) is used to estimate the robot's current pose (position and orientation).
‚Ä¢	Goal: The primary goal of the low-level control is to ensure smooth, accurate, and stable robot motion according to the commands received from the mid-level controller.
2. Mid-Level Control: Reactive Navigation and Safety
The mid-level control system builds upon the capabilities of the low-level layer, focusing on the robot's interaction with its immediate surroundings and ensuring safe navigation.
‚Ä¢	Obstacle Avoidance: This is a critical function of the mid-level control. By processing real-time data from the robot's suite of sensors (LiDAR, depth camera, ultrasonic sensors), this layer detects obstacles in the robot's path.
‚Ä¢	Reactive Behaviors: Based on the sensor data, the mid-level controller implements reactive behaviors to avoid collisions. These behaviors might include slowing down, stopping, turning, or maneuvering around obstacles. Algorithms like Vector Field Histogram (VFH), Dynamic Window Approach (DWA), or similar reactive planning techniques are typically employed.
3. High-Level Autonomy: Intelligent Mission Execution
The high-level autonomy system orchestrates the robot's overall mission, enabling it to perform complex tasks without continuous human intervention.
‚Ä¢	Patrol Routing: This layer utilizes a pre-mapped representation of the warehouse, dividing it into logical zones or waypoints. Based on the desired patrol strategy (e.g., sequential zone coverage, prioritized areas), the high-level controller generates a sequence of target locations or paths for the robot to follow.
‚Ä¢	Behavior Trees: Behavior trees provide a powerful and modular framework for defining the robot's high-level behaviors and decision-making processes. They allow for the representation of complex sequences, conditional actions, and parallel tasks involved in the patrolling mission (e.g., navigate to zone A, scan for anomalies, log data, proceed to zone B).
‚Ä¢	ROS2 Navigation Stack Integration: The robot leverages the ROS2 Navigation stack, a robust and widely used framework for mobile robot navigation. This stack provides pre-built functionalities for path planning, obstacle avoidance, localization, and map management.
‚Ä¢	Autonomous Decision-Making: The high-level system enables the robot to make autonomous decisions regarding patrol routes based on factors like time, priority, or previously detected anomalies.
‚Ä¢	Dynamic Path Re-planning: If the mid-level control encounters an unforeseen obstacle that blocks the planned path, the high-level autonomy system can trigger path re-planning. Utilizing the current map and the robot's current location, the Navigation stack can compute a new, feasible path to reach the original goal or the next patrol waypoint.
‚Ä¢	Anomaly Detection Integration: This layer will also integrate with the robot's perception system to process anomaly detection results. Upon detecting an anomaly, the high-level controller can deviate from the standard patrol route to investigate, log the event, and potentially trigger alerts.
In summary, the layered control and autonomy system allows the warehouse patrolling robot to:
‚Ä¢	Execute precise movements based on high-level commands (low-level control).
‚Ä¢	Navigate safely and reactively to dynamic environments by avoiding obstacles (mid-level control).
‚Ä¢	Intelligently plan and execute patrol missions based on pre-defined strategies and adapt to changing circumstances (high-level autonomy).
‚Ä¢	Leverage the power of ROS2 Navigation for robust path planning and navigation capabilities.
This sophisticated control architecture ensures that the robot can effectively and efficiently perform its warehouse patrolling tasks, enhancing security, safety, and operational awareness within the facility.
________________________________________
Preparation Needs
To succeed, we need a deeper understanding of:
‚Ä¢	Multi-sensor fusion in ROS2
‚Ä¢	Navigation stack configuration and tuning
‚Ä¢	Behavior trees and FSMs in autonomous systems
‚Ä¢	GUI development using rqt and Python Qt
Class Topics Requested:
‚Ä¢	Detailed coverage of ROS2 Navigation stack
‚Ä¢	Real-time system debugging techniques
‚Ä¢	Sensor data filtering and fusion strategies
________________________________________
Final Demonstration Plan
Our final demonstration is designed to showcase the full capabilities of the Autonomous Warehouse Patrolling Robot, developed using the TurtleBot4 platform and ROS2. The demo will simulate a realistic indoor warehouse environment using a scaled mockup constructed within a classroom. This hands-on trial will allow observers to evaluate the robot‚Äôs ability to autonomously patrol, avoid obstacles, detect anomalies, and present live system data through a custom-built graphical user interface (GUI).
________________________________________
üß™ Demo Description
During the demonstration, TurtleBot4 will autonomously navigate through a mock warehouse layout configured using tables, cardboard boxes, and tape-marked pathways. It will:
‚Ä¢	Perform a patrol loop based on predefined waypoints.
‚Ä¢	Use sensor data from LiDAR, depth camera, IMU, and ultrasonic sensors to perceive and react to the environment.
‚Ä¢	Avoid obstacles (both static and dynamic) in real-time.
‚Ä¢	Provide live telemetry and visualization on a projector-based GUI that displays:
o	Real-time map (from RViz)
o	Robot position and path
o	Obstacle alerts
o	Anomaly flags (e.g., unexpected objects or human detection)
The demo will highlight key features such as localization accuracy, real-time responsiveness, safety, and user interaction.
________________________________________
üõ†Ô∏è Resources Needed
To conduct the final demonstration, we will require the following components:
‚Ä¢	TurtleBot4 equipped with LiDAR, depth camera, IMU, and onboard compute.
‚Ä¢	A pre-generated warehouse map loaded into RViz for AMCL-based localization.
‚Ä¢	Mock warehouse setup: Classroom space arranged with:
o	Tables and cardboard boxes to simulate shelving.
o	Clear lanes representing robot patrol paths.
‚Ä¢	Wi-Fi router for enabling ROS2 multi-node communication over the network.
‚Ä¢	A projector and monitor setup to display the custom GUI for real-time interaction and visualization.
________________________________________
üèóÔ∏è Classroom Setup
‚Ä¢	We will create aisles between tables and boxes to simulate warehouse lanes.
‚Ä¢	"Anomaly zones" will be marked with boxes or human presence for the robot to detect and log.
‚Ä¢	The TurtleBot4 will follow a set patrol route defined in the map and updated via the navigation stack.
‚Ä¢	A central station (laptop or workstation) will run the GUI and visualization tools, connected to the robot over Wi-Fi.
________________________________________
üåç Environmental Adaptability
To ensure robustness and adaptability in a non-static environment:
‚Ä¢	AMCL (Adaptive Monte Carlo Localization) will handle localization uncertainty by continuously updating the robot‚Äôs pose using LiDAR scans and the known map.
‚Ä¢	Dynamic costmaps in the navigation stack will enable the robot to avoid moving obstacles (e.g., people or displaced objects) and re-route in real time.
‚Ä¢	Sensor data fusion ensures redundancy and resilience if one sensor becomes noisy or fails temporarily.
________________________________________
‚úÖ Testing & Evaluation Plan
To verify system readiness and ensure a successful demo, we have implemented a multi-tiered testing framework:
1.	Unit Testing:
o	Validate functionality of individual ROS2 nodes, including sensor input processing, map generation, and GUI backend.
2.	Functional Testing:
o	Test integration between navigation, sensor fusion, and GUI components.
o	Validate that anomaly detection correctly flags unexpected objects or people.
3.	Live Evaluation Metrics:
o	Coverage Assessment: Ensure the robot completes a full patrol loop and visits all waypoints.
o	Anomaly Detection Accuracy: Compare flagged anomalies to actual placements.
o	Obstacle Avoidance Response Time: Measure latency in responding to new obstacles.
o	GUI Usability: Confirm that the GUI provides clear, real-time, actionable feedback for users.
Video recording and real-time logging will be used to analyze performance and validate mission success.
________________________________________
Impact
This project pushes us to explore real-world robotic deployments using ROS2, a critical skill for modern robotic systems. It encourages learning across:
‚Ä¢	Sensor integration
‚Ä¢	Autonomous control
‚Ä¢	System design
‚Ä¢	GUI development It could influence future coursework by offering a modular demo project template. It will also prepare us for robotics research or industrial roles in automation and logistics.
________________________________________
Advising
‚Ä¢	Advisor: Dr. Aukes
‚Ä¢	Requested Resources:
o	Access to TurtleBot4
o	Weekly project check-ins
o	Use of robotics lab for after-hours testing
‚Ä¢	Advisor Expectations:
o	Demonstrable progress every 2 weeks
o	Functional final demo
o	Full integration of perception, planning, and interaction modules
________________________________________

Weekly Milestones (Gantt/Table Format)
Week	Hardware Integration	Interface Development	Sensors	Controls & Autonomy
7	Initial TurtleBot4 setup	GUI layout skeleton	Verify LiDAR and camera streams	Basic teleoperation setup
8	Connect all sensors	GUI live sensor stream	Depth camera calibration	Low-level motor control
9	Odometry testing	Map view integration	IMU testing and filtering	PID tuning
10	SLAM with LiDAR	Add patrol status display	Sensor fusion trial	Path planning trial
11	Warehouse map creation	Implement alert logging	Dynamic obstacle detection	Obstacle avoidance tuning
12	Integrate patrol behavior	Add user override	Human/object detection using depth	FSM or Behavior Tree setup
13	System testing in lab	GUI final integration	Real-world condition testing	Re-planning and fallback logic
14	Demo dry-run #1	Final visual polish	Final filtering refinements	Robustness testing
15	Debugging and patching	Optimize performance	Sensor noise reduction	Emergency stop tuning
16	Final demonstration prep	Presentation visuals	Run-time logs and metrics export	Final behavior integration
________________________________________
