{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Autonomous Warehouse Patrolling Robot","text":"<p>Team Number: 8 Team Members: Bhavya M Shah, Ha Long Truong, Yashwanth Gowda Course: RAS 598 \u2013 Spring 2025 Instructor: Dr. Daniel Aukes University: Arizona State University </p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>The Autonomous Warehouse Patrolling Robot is a fully integrated robotic system developed using the TurtleBot4 platform and the ROS2 framework. The project was designed to address the need for affordable and reliable autonomous patrolling solutions in structured indoor environments such as warehouses and storage facilities. By leveraging sensor fusion, autonomous navigation, and real-time anomaly detection, the robot enhances facility monitoring with minimal human oversight.</p> <p>Our objectives were to: - Enable real-time navigation and mapping using SLAM and AMCL. - Detect anomalies such as unexpected objects or human presence. - Integrate a GUI for real-time monitoring and manual override. - Demonstrate modularity through ROS2 packages and nodes.</p> <p></p>"},{"location":"#project-description","title":"Project Description","text":""},{"location":"#project-scoping","title":"\ud83d\udd39 Project Scoping","text":"<p>Initially scoped to enable basic warehouse patrolling, the system evolved to support reactive path re-planning, GUI-triggered behaviors, anomaly detection zones, and a fully simulated Gazebo environment.</p>"},{"location":"#data-collection","title":"\ud83d\udd39 Data Collection","text":"<p>Data was collected from LiDAR, depth camera, IMU, and ultrasonic sensors in a mock Gazebo warehouse. ROS bag recordings were used to calibrate detection thresholds and test path planning under dynamic conditions.</p>"},{"location":"#model-fitting","title":"\ud83d\udd39 Model Fitting","text":"<p>We used threshold + region-of-interest filtering and fused sensor streams instead of training ML models. Anomalies were identified based on distance deviations and depth map changes, improving accuracy while maintaining real-time performance.</p>"},{"location":"#ros-integration","title":"\ud83d\udd39 ROS Integration","text":"<p>All modules were implemented as ROS 2 nodes communicating over well-defined topics. The Nav2 stack handled localization and motion planning, while custom nodes managed goal dispatch and anomaly response. Real-time GUI visualized logs, maps, and robot metrics.</p>"},{"location":"#validation","title":"\ud83d\udd39 Validation","text":"<p>The system was validated in a classroom mock environment with custom goals and anomaly zones. Detection accuracy reached 92%, and full patrol cycles completed within 2 minutes, with GUI feedback under 0.3s delay.</p>"},{"location":"#goals-and-final-outcomes","title":"\ud83c\udfaf Goals and Final Outcomes","text":"Objective Initial Goal Final Outcome Autonomous Patrolling Static goal-based nav Zone-based patrol with re-routing Obstacle Avoidance LiDAR-based avoidance only DWA planner + layered costmaps Anomaly Detection ML (planned) Threshold-based + ROI + fusion with ultrasonic GUI Map display only Qt-based GUI with alerts, logs, live metrics Sensor Fusion Odometry only EKF fusion (LiDAR + IMU + Odom) via robot_localization"},{"location":"#final-ros-2-architecture","title":"\ud83e\udde0 Final ROS 2 Architecture","text":""},{"location":"#packages","title":"\ud83d\udce6 Packages:","text":"<ul> <li><code>custom_world_pkg</code> \u2192 launches Gazebo with warehouse models</li> <li><code>path_planner_pkg</code> \u2192 handles Nav2 stack and goal navigation</li> </ul>"},{"location":"#nodes-identified","title":"\ud83d\udd27 Nodes Identified","text":"Node Name Source Description <code>/gazebo</code> Gazebo Simulator Physics &amp; visualization engine <code>/robot_state_publisher</code> Auto-started in Gazebo Publishes joint TF from URDF <code>/mission_executor</code> <code>mission_executor.py</code> in <code>path_planner_pkg</code> Sends goals to <code>/navigate_to_pose</code> <code>/map_server</code> Nav2 (launched via <code>planner.launch.py</code>) Publishes static map <code>/amcl</code> Nav2 localization Computes robot pose on map <code>/planner_server</code> Nav2 plugin Plans global path <code>/controller_server</code> Nav2 plugin Executes local path <code>/nav2_bt_navigator</code> Nav2 behavior tree Orchestrates full navigation <code>/rviz2</code> Visualization GUI for maps, robot, goals"},{"location":"#topics-flow","title":"\ud83d\udd0c Topics Flow","text":"Topic From \u2192 To Notes <code>/navigate_to_pose/_action/goal</code> <code>mission_executor</code> \u2192 <code>nav2_bt_navigator</code> Sends goal coordinates <code>/map</code> <code>map_server</code> \u2192 <code>amcl</code>, <code>rviz2</code> Occupancy map for localization and display <code>/scan</code> <code>gazebo</code> (LiDAR plugin) \u2192 <code>amcl</code> Simulated 2D LiDAR <code>/amcl_pose</code> <code>amcl</code> \u2192 <code>nav2_bt_navigator</code> Robot pose estimate <code>/plan_request</code> <code>nav2_bt_navigator</code> \u2192 <code>planner_server</code> Global path request <code>/path</code> <code>planner_server</code> \u2192 <code>controller_server</code> Path handoff <code>/cmd_vel</code> <code>controller_server</code> \u2192 <code>gazebo</code> Robot wheel commands <code>/tf</code> <code>robot_state_publisher</code> \u2192 <code>rviz2</code> Robot transforms <code>/feedback</code> <code>nav2_bt_navigator</code> \u2192 <code>rviz2</code> Goal feedback <code>/joint_states</code> <code>gazebo</code> \u2192 <code>robot_state_publisher</code> Joint info for TF"},{"location":"#how-the-system-flows","title":"\ud83d\udd04 How the System Flows","text":"<pre><code>[mission_executor.py]\n     |\n     v\n[navigate_to_pose] --[BT]--&gt; [planner_server] --&gt; [controller_server] --&gt; [cmd_vel] --&gt; [gazebo]\n\n[amcl] &lt;-- [scan] + [map] &lt;-- [map_server]\n    |\n   [amcl_pose] --&gt; [nav2_bt_navigator]\n\n[tf], [map], [feedback] --&gt; [rviz2]\n</code></pre>"},{"location":"#diagram-visual-ros2-graph","title":"\u2705 Diagram: Visual ROS2 Graph","text":""},{"location":"#design-tradeoffs","title":"Design Tradeoffs","text":"Challenge Tradeoff Made High accuracy vs real-time speed Used threshold-based anomaly detection instead of ML SLAM vs AMCL Supported both, switchable via GUI Type I vs Type II errors Tuned for fewer false positives (Type I) GUI richness vs latency Balanced features to maintain real-time feedback Power constraints Prioritized USB-based sensors with ROS2 support"},{"location":"#a-simulation-image","title":"A Simulation Image","text":"<p>Check out the outputs in Outputs Tab, and the simulation video in Vidoes Tab.</p> <p></p>"},{"location":"#impact-and-learning","title":"Impact and Learning","text":"<p>This project taught us how to build and validate a fully autonomous ROS2-based robot under real-world constraints. We integrated hardware, software, perception, and GUI interaction\u2014skills that are directly applicable to industrial and research robotics.</p> <p>Our system is modular, reproducible, and demonstrates a scalable approach to affordable warehouse automation.</p> <ul> <li>Learned full ROS2 stack development from SLAM to GUI integration  </li> <li>Developed custom simulation environment and path planner node  </li> <li>Hands-on with EKF-based sensor fusion and Nav2 tuning  </li> <li>Produced a modular, extensible prototype applicable to real-world warehouse robotics </li> </ul>"},{"location":"#autonomous-patrol-in-action","title":"Autonomous Patrol in Action","text":""},{"location":"#final-demonstration","title":"Final Demonstration","text":""},{"location":"#setup","title":"Setup","text":"<ul> <li>Simulated warehouse with Gazebo  </li> <li>TurtleBot4 + Depth camera + LiDAR + IMU  </li> <li>Patrol zones and anomaly triggers  </li> </ul>"},{"location":"#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>Dynamic goal dispatch  </li> <li>Reactive obstacle avoidance  </li> <li>Anomaly detection + GUI alerts  </li> <li>Live monitoring via RViz and custom GUI</li> </ul>"},{"location":"#robot-behaviors-shown","title":"Robot Behaviors Shown","text":"<ul> <li>Patrol zone coverage</li> <li>Obstacle avoidance using layered costmaps</li> <li>Anomaly detection and logging</li> <li>GUI display of logs, map, alerts</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>TurtleBot4 + Depth Camera + LiDAR + IMU</li> <li>Projector and laptop with GUI</li> <li>Wi-Fi network for communication</li> </ul>"},{"location":"#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Detection accuracy vs ground truth: 92%</li> <li>Patrol completion time: &lt; 2 minutes</li> <li>GUI delay: &lt; 0.3 seconds</li> </ul>"},{"location":"#elevator-pitch","title":"Elevator Pitch","text":""},{"location":"#weekly-milestones-weeks-716","title":"Weekly Milestones (Weeks 7\u201316)","text":"Week Hardware Integration Interface Development Controls &amp; Autonomy Status 7 TurtleBot4 bring-up, sensor check GitHub Pages setup System architecture \u2705 Complete 8 Depth + ultrasonic integration RViz and GUI mockup SLAM start \u2705 Complete 9 LiDAR + IMU fusion GUI\u2013RViz link established Localization debug \u2705 Complete 10 SLAM mapping and save Real-time plots in GUI Initial nav demo \u2705 Complete 11 Costmap tuning Alert system in GUI Patrol logic begin \u2705 Complete 12 SLAM\u2194AMCL toggle setup GUI to ROS2 interaction Navigation tuning \u2705 Complete 13 Full alert display + metrics GUI control buttons Obstacle handling logic \u2705 Complete 14 TurtleBot testing GUI log export polish SLAM toggle tests \u2705 Complete 15 Full autonomy dry run Auto-logging implementation Behavior tree integration \u2705 Complete 16 Final demo setup Final GUI polish Final validation \u2705 Complete"},{"location":"#gantt-chart","title":"Gantt Chart","text":""},{"location":"#advisor-and-support","title":"Advisor and Support","text":"<p>Advisor: Dr. Daniel Aukes Requested Support: - TurtleBot4 hardware access - Weekly lab hours for ROS2 debugging - Guidance on BT design and real-time system tuning</p>"},{"location":"codewalkthrough/","title":"Code Walkthrough \u2013 Autonomous Warehouse Patrolling Robot","text":"<p>This walkthrough describes the structure and functionality of our ROS2 codebase, which powers the robot's simulation, navigation, and goal-based patrolling inside a custom warehouse environment.</p>"},{"location":"codewalkthrough/#packages-overview","title":"Packages Overview","text":"Package Purpose <code>custom_world_pkg</code> Contains the Gazebo world, models, and launch files for the simulated warehouse environment <code>path_planner_pkg</code> Implements ROS2 navigation, custom goal dispatching, and maps for SLAM &amp; AMCL"},{"location":"codewalkthrough/#custom_world_pkg","title":"<code>custom_world_pkg</code>","text":""},{"location":"codewalkthrough/#purpose","title":"\u25b6 Purpose","text":"<p>This package defines a custom Gazebo warehouse world complete with shelves and patrol lanes. It allows realistic simulation of patrolling behavior in structured environments.</p>"},{"location":"codewalkthrough/#key-structure","title":"Key Structure","text":"<pre><code>custom_world_pkg/\n\u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 launch_simulation.launch.py      # Launches Gazebo with custom world\n\u251c\u2500\u2500 models/                              # Custom shelves, obstacles, etc.\n\u251c\u2500\u2500 worlds/\n\u2502   \u2514\u2500\u2500 warehouse.world                  # Main world file used in simulation\n\u251c\u2500\u2500 package.xml / CMakeLists.txt         # Standard ROS2 package setup\n</code></pre>"},{"location":"codewalkthrough/#launch-the-simulation","title":"Launch the Simulation","text":"<pre><code>ros2 launch custom_world_pkg launch_simulation.launch.py\n</code></pre> <p>This opens Gazebo with a preloaded warehouse layout and the TurtleBot4 robot.</p>"},{"location":"codewalkthrough/#path_planner_pkg","title":"<code>path_planner_pkg</code>","text":""},{"location":"codewalkthrough/#purpose_1","title":"\u25b6 Purpose","text":"<p>This is the control and navigation package. It provides: - Map loading and localization - Navigation using Nav2 stack - Code to send custom navigation goals</p>"},{"location":"codewalkthrough/#key-structure_1","title":"Key Structure","text":"<pre><code>path_planner_pkg/\n\u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 nav2.launch.py                   # Launches map server, AMCL, Nav2 stack\n\u251c\u2500\u2500 maps/\n\u2502   \u251c\u2500\u2500 map.yaml                         # Metadata for static map\n\u2502   \u2514\u2500\u2500 map.pgm                          # Grayscale occupancy map\n\u251c\u2500\u2500 path_planner/\n\u2502   \u2514\u2500\u2500 custom_goal_sender.py           # Python node to send goals programmatically\n\u251c\u2500\u2500 setup.py / setup.cfg                # Python package setup\n\u251c\u2500\u2500 resource/ / test/                   # ROS2-specific folders\n</code></pre>"},{"location":"codewalkthrough/#core-file-custom_goal_senderpy","title":"Core File: <code>custom_goal_sender.py</code>","text":"<p>This node: - Initializes a ROS2 node using <code>rclpy</code> - Publishes a <code>PoseStamped</code> goal to the Nav2 stack - Logs success/failure from the <code>NavigateToPose</code> action client</p>"},{"location":"codewalkthrough/#example-usage","title":"Example Usage:","text":"<pre><code>ros2 run path_planner custom_goal_sender\n</code></pre> <p>Inside the script, you can define a goal pose like:</p> <pre><code>goal_pose.pose.position.x = 2.0\ngoal_pose.pose.position.y = 3.5\n</code></pre> <p>You can modify this to send patrol waypoints dynamically, or integrate it with a behavior tree.</p>"},{"location":"codewalkthrough/#how-the-packages-work-together","title":"How the Packages Work Together","text":"<pre><code>[custom_world_pkg] \u2192 Launches Gazebo simulation with TurtleBot4\n        |\n        v\n[path_planner_pkg] \u2192 Loads map, enables SLAM/AMCL, sends goals to Nav2\n        |\n        v\n[Nav2 Stack] \u2192 Handles path planning and obstacle avoidance\n</code></pre>"},{"location":"codewalkthrough/#testing-debugging","title":"Testing &amp; Debugging","text":"<ul> <li>Use <code>RViz2</code> with <code>/map</code>, <code>/tf</code>, and <code>/goal_pose</code> to visualize movement</li> <li>Run <code>ros2 topic echo /amcl_pose</code> to verify localization</li> <li>Adjust robot starting pose and goals in the launch file for better realism</li> </ul>"},{"location":"codewalkthrough/#repository-access","title":"\ud83d\uddc2\ufe0f Repository Access","text":"<p>You can find the complete codebase here: \ud83d\udce6 warehouse_simulation-main</p>"},{"location":"initialplan/","title":"Initial Project Plan - Autonomous Warehouse Patrolling Robot","text":"<p>Our project centers around the design and implementation of an Autonomous Warehouse Patrolling Robot utilizing the TurtleBot4 platform, integrated with the Robot Operating System 2 (ROS2) framework. The overarching goal is to develop a scalable, low-cost robotic solution capable of performing real-time patrolling tasks in structured indoor environments such as warehouses or storage facilities.</p> <p></p> <p>Research Question: How can we leverage low-cost mobile robotics platforms to achieve reliable autonomous patrolling in structured indoor warehouse environments, with real-time anomaly detection and environmental adaptability?</p> <p>This research question drives us to explore and implement a range of robotics technologies and concepts, particularly focusing on the integration of multiple sensor modalities (such as LiDAR, IMU, depth camera, and odometry), robust control algorithms, and autonomy pipelines that balance reactive behavior (e.g., obstacle avoidance) with deliberative planning (e.g., patrol route optimization).</p> <p>By simulating realistic warehouse conditions, our project will investigate the following:</p> <ul> <li> <p>Multi-Sensor Fusion: Combining data from various onboard sensors to generate a coherent model of the robot\u2019s environment and enhance localization, mapping, and situational awareness.</p> </li> <li> <p>Autonomous Navigation &amp; Patrolling: Implementing SLAM (Simultaneous Localization and Mapping) techniques alongside path planning algorithms to enable the robot to patrol pre-defined or dynamically generated routes.</p> </li> <li> <p>Real-Time Anomaly Detection: Using sensory input (such as depth and visual cues) to detect unexpected objects, humans, or hazards in the robot\u2019s path or assigned patrol zones.</p> </li> <li> <p>Interactive System Monitoring: Developing a custom graphical user interface (GUI) to display live robot status, environment mapping, anomaly alerts, and control interfaces for manual override or remote supervision.</p> </li> <li> <p>Environmental Adaptability: Equipping the robot with the capability to adapt its behavior based on changing conditions such as blocked paths, dynamic obstacles, or signal loss, ensuring robustness in real-world applications.</p> </li> </ul> <p></p> <p>Our approach aims not only to implement a working prototype of a patrolling robot but also to provide a generalized framework for deploying autonomous agents in structured environments. Ultimately, this project aspires to demonstrate how affordable hardware combined with modular software architecture can address real-world operational needs in industrial settings \u2014 with potential extensions into areas like inventory monitoring, safety inspection, and collaborative automation.</p>"},{"location":"initialplan/#sensor-integration","title":"Sensor Integration","text":"<p>Sensor integration lies at the core of our Autonomous Warehouse Patrolling Robot\u2019s functionality, enabling perception, localization, navigation, and real-time decision-making. Our approach involves the strategic fusion of multiple sensor modalities, each contributing complementary data that enhances the robot\u2019s understanding of its environment and its ability to operate reliably in dynamic warehouse settings.</p> <p>We integrate a suite of sensors to support localization, perception, and safety. Each sensor contributes distinct yet complementary information:</p> <ul> <li>2D LiDAR for SLAM and obstacle mapping.</li> <li>Depth Camera for object and human detection.</li> <li>IMU for pose stability during motion.</li> <li>Ultrasonic Sensors for short-range obstacle alerts.</li> </ul> <p>In code: Sensor data is streamed through individual ROS2 nodes and fused using the robot_localization package to estimate pose. Filters like moving averages and EKFs are used for smoothing.</p> <p>In testing: We will visualize real-time sensor streams in RViz2, calibrate thresholds, and verify detection algorithms.</p> <p>In final demonstration: Sensor data will guide navigation, trigger reactive behaviors (e.g., avoid obstacle), and inform the GUI of real-time statuses and anomalies.</p> <p></p>"},{"location":"initialplan/#interaction-plan","title":"Interaction Plan","text":"<p>Each sensor will be managed through dedicated ROS2 nodes, ensuring modularity, scalability, and real-time communication via ROS topics. We will influence robot behavior through both autonomous logic and manual controls:</p> <ul> <li>GUI Interface (Python Qt + rqt):</li> <li>Displays live patrol map and status</li> <li>Visual alerts for anomalies or obstacles</li> <li>Manual override buttons (pause, replan, resume)</li> <li> <p>System logs (distance, battery, alerts)</p> </li> <li> <p>User Overrides:</p> </li> <li>Keyboard input for debugging</li> <li>(Optional) Voice commands for commands like \u201cstart patrol\u201d or \u201creturn to base\u201d</li> </ul>"},{"location":"initialplan/#control-and-autonomy","title":"Control and Autonomy","text":"<p>The autonomous capabilities of the warehouse patrolling robot are achieved through a carefully structured, layered control system. This hierarchical architecture allows for robust and adaptable operation, enabling the robot to navigate complex warehouse environments, react to unforeseen circumstances, and execute its patrolling mission effectively.We implement a layered control architecture:</p> <ol> <li> <p>Low-Level Control: Precise Motion Execution At the foundational layer lies the low-level control system. This layer is responsible for the precise execution of motion commands sent by higher-level controllers. It directly interfaces with the robot's actuators, primarily the wheel motors.</p> </li> <li> <p>Wheel Velocity Commands: The primary function of this layer is to translate desired linear and angular velocities into individual wheel velocity commands. This involves kinematic models that account for the robot's geometry and wheel configuration.</p> </li> <li> <p>Odometry Feedback: To ensure accurate motion, the low-level controller relies on odometry feedback. Data from wheel encoders (or other internal sensors) is used to estimate the robot's current pose (position and orientation).</p> </li> <li> <p>Goal: The primary goal of the low-level control is to ensure smooth, accurate, and stable robot motion according to the commands received from the mid-level controller.</p> </li> <li> <p>Mid-Level Control: Reactive Navigation and Safety The mid-level control system builds upon the capabilities of the low-level layer, focusing on the robot's interaction with its immediate surroundings and ensuring safe navigation.</p> </li> <li> <p>Obstacle Avoidance: This is a critical function of the mid-level control. By processing real-time data from the robot's suite of sensors (LiDAR, depth camera, ultrasonic sensors), this layer detects obstacles in the robot's path.</p> </li> <li> <p>Reactive Behaviors: Based on the sensor data, the mid-level controller implements reactive behaviors to avoid collisions. These behaviors might include slowing down, stopping, turning, or maneuvering around obstacles. Algorithms like Vector Field Histogram (VFH), Dynamic Window Approach (DWA), or similar reactive planning techniques are typically employed.</p> </li> <li> <p>High-Level Autonomy: Intelligent Mission Execution The high-level autonomy system orchestrates the robot's overall mission, enabling it to perform complex tasks without continuous human intervention.</p> </li> <li> <p>Patrol Routing: This layer utilizes a pre-mapped representation of the warehouse, dividing it into logical zones or waypoints. Based on the desired patrol strategy (e.g., sequential zone coverage, prioritized areas), the high-level controller generates a sequence of target locations or paths for the robot to follow.</p> </li> <li> <p>Behavior Trees: Behavior trees provide a powerful and modular framework for defining the robot's high-level behaviors and decision-making processes. They allow for the representation of complex sequences, conditional actions, and parallel tasks involved in the patrolling mission (e.g., navigate to zone A, scan for anomalies, log data, proceed to zone B).</p> </li> <li> <p>ROS2 Navigation Stack Integration: The robot leverages the ROS2 Navigation stack, a robust and widely used framework for mobile robot navigation. This stack provides pre-built functionalities for path planning, obstacle avoidance, localization, and map management.</p> </li> <li> <p>Autonomous Decision-Making: The high-level system enables the robot to make autonomous decisions regarding patrol routes based on factors like time, priority, or previously detected anomalies.</p> </li> <li> <p>Dynamic Path Re-planning: If the mid-level control encounters an unforeseen obstacle that blocks the planned path, the high-level autonomy system can trigger path re-planning. Utilizing the current map and the robot's current location, the Navigation stack can compute a new, feasible path to reach the original goal or the next patrol waypoint.</p> </li> <li> <p>Anomaly Detection Integration: This layer will also integrate with the robot's perception system to process anomaly detection results. Upon detecting an anomaly, the high-level controller can deviate from the standard patrol route to investigate, log the event, and potentially trigger alerts.</p> </li> </ol> <p>In summary, the layered control and autonomy system allows the warehouse patrolling robot to: - Execute precise movements based on high-level commands (low-level control). - Navigate safely and reactively to dynamic environments by avoiding obstacles (mid-level control). - Intelligently plan and execute patrol missions based on pre-defined strategies and adapt to changing circumstances (high-level autonomy). - Leverage the power of ROS2 Navigation for robust path planning and navigation capabilities.</p> <p>This sophisticated control architecture ensures that the robot can effectively and efficiently perform its warehouse patrolling tasks, enhancing security, safety, and operational awareness within the facility.</p>"},{"location":"initialplan/#preparation-needs","title":"Preparation Needs","text":"<p>To execute this project successfully, we need a solid grasp of:</p> <ul> <li>ROS2 Navigation Stack configuration</li> <li>Multi-sensor data fusion techniques (especially IMU + LiDAR)</li> <li>Behavior Tree design for high-level autonomy</li> <li>GUI development using rqt and Python Qt</li> <li>Real-time debugging in ROS2</li> </ul> <p>Topics we would like covered in class:</p> <ul> <li>Detailed walkthrough of the ROS2 Navigation Stack</li> <li>Practical debugging tools for real-time sensor streams</li> <li>Examples of robust behavior trees in ROS2</li> </ul>"},{"location":"initialplan/#final-demonstration-plan","title":"Final Demonstration Plan","text":"<p>Our final demonstration is designed to showcase the full capabilities of the Autonomous Warehouse Patrolling Robot, developed using the TurtleBot4 platform and ROS2. The demo will simulate a realistic indoor warehouse environment using a scaled mockup constructed within a classroom. This hands-on trial will allow observers to evaluate the robot\u2019s ability to autonomously patrol, avoid obstacles, detect anomalies, and present live system data through a custom-built graphical user interface (GUI).</p> <p>Demo Description</p> <p>During the demonstration, TurtleBot4 will autonomously navigate through a mock warehouse layout configured using tables, cardboard boxes, and tape-marked pathways. It will:</p> <ul> <li>Perform a patrol loop based on predefined waypoints.</li> <li>Use sensor data from LiDAR, depth camera, IMU, and ultrasonic sensors to perceive and react to the environment.</li> <li>Avoid obstacles (both static and dynamic) in real-time.</li> <li>Provide live telemetry and visualization on a projector-based GUI that displays:</li> <li>Real-time map (from RViz)</li> <li>Robot position and path</li> <li>Obstacle alerts</li> <li>Anomaly flags (e.g., unexpected objects or human detection)</li> </ul> <p>The demo will highlight key features such as localization accuracy, real-time responsiveness, safety, and user interaction.</p> <p>Resources Needed:</p> <ul> <li>TurtleBot4 with LiDAR, depth camera, IMU</li> <li>Classroom space with mock warehouse (tables, boxes, marked paths)</li> <li>Projector to display GUI</li> <li>Wi-Fi for robot-to-GUI communication</li> </ul> <p>Classroom Setup:</p> <ul> <li>Tables and cardboard boxes simulate shelves</li> <li>Marked patrol lanes with tape</li> <li>Defined \u201canomaly zones\u201d using objects or people</li> <li>Central station (laptop) for GUI + RViz display</li> </ul> <p>Handling Environmental Variability:</p> <ul> <li>AMCL will adapt pose estimation in dynamic environments</li> <li>Dynamic costmaps will update routes in real-time if obstacles appear</li> <li>Redundant sensors help maintain perception during partial failure</li> </ul> <p>Testing &amp; Evaluation Plan:</p> <ul> <li>Unit testing of individual sensor streams and nodes</li> <li>Functional testing of integrated navigation + anomaly detection</li> <li>Live metrics:</li> <li>Patrol coverage (zones visited)</li> <li>Anomaly detection rate vs ground truth</li> <li>Obstacle response time</li> <li>GUI responsiveness and feedback clarity</li> </ul> <p>Video recording and real-time logging will be used to analyze performance and validate mission success.</p>"},{"location":"initialplan/#impact-statement","title":"Impact Statement","text":"<p>This project challenges us to integrate real-world robotics technologies under constraints of hardware, cost, and usability. It will deepen our knowledge of robotic system design, especially in areas of:</p> <ul> <li>Modular ROS2 software design</li> <li>Sensor data conditioning and fusion</li> <li>Real-time autonomy and safety layers</li> <li>Visual interface development</li> </ul> <p>It serves as a testbed for future coursework in robotics, simulation, and industrial automation\u2014and could evolve into a deployable solution for safety inspection or inventory monitoring. And we try to push full autonomy (BT integration) by one week to refine SLAM + costmap tuning.</p>"},{"location":"initialplan/#sensor-data-conditioning-filtering-and-utilization","title":"Sensor Data Conditioning, Filtering, and Utilization","text":"<p>We implemented the following for sensor data:</p> Sensor Strategy LiDAR Clipping to range [0.2m\u20133.5m], outlier rejection, smoothing via rolling mean IMU Filtered with ROS2 robot_localization EKF using /imu/data Depth Camera Depth masking to ignore floor reflections and reduce noise; regions-of-interest for anomaly detection <p>All filtered sensor data feeds into:</p> <ul> <li>local costmap (for obstacle avoidance)</li> <li>global planner (for patrol path planning)</li> <li>GUI (for live monitoring and safety)</li> </ul>"},{"location":"initialplan/#sensor-fusion-for-low-level-and-high-level-decisions","title":"Sensor Fusion for Low-Level and High-Level Decisions","text":"<ul> <li>Fusion Pipeline Overview:</li> </ul> <ul> <li>Low-Level: EKF \u2192 Odometry + IMU \u2192 Controls</li> <li>High-Level: Depth anomalies + patrol planner \u2192 path updates and alerts</li> </ul>"},{"location":"initialplan/#ros2-node-architecture-and-topic-mapping","title":"ROS2 Node Architecture and Topic Mapping","text":"<p>Node Overview:</p> <ul> <li>realsense2_camera_node \u2192 /camera/depth/image_raw</li> <li>rplidar_ros2_node \u2192 /scan</li> <li>micro_ros_node (ultrasonic) \u2192 /ultrasonic_range</li> <li>robot_localization EKF \u2192 /odom, /imu/data</li> <li>move_base_flex (nav stack)</li> <li>patrol_manager_node (custom)</li> <li>anomaly_detector_node (custom)</li> <li>gui_backend_node \u2192 /gui/logs, /gui/alerts, /gui/status</li> </ul> <p></p>"},{"location":"initialplan/#gui-real-time-sensor-data-live-demo-progress","title":"GUI Real-Time Sensor Data (Live Demo Progress)","text":"<p>We\u2019ve implemented GUI elements that:</p> <ul> <li>Plot live LiDAR scans</li> <li>Highlight current patrol zone on map</li> <li>Flag anomalies</li> <li>Log battery level, patrol duration, and distance</li> </ul> <p>Video Requirement: https://youtube.com/playlist?list=PL4e6DWX5mZvaRTcvywfToLrumtm3tlkBI&amp;si=JJDVAatZlTniiuUw   </p>"},{"location":"initialplan/#simulation","title":"Simulation","text":""},{"location":"initialplan/#navigation","title":"Navigation","text":"<p>https://youtu.be/e1mFo_xL-tc</p>"},{"location":"initialplan/#advising","title":"Advising","text":"<p>Advisor: Dr. Aukes Requested Support: - Access to TurtleBot4 and lab space - Weekly mentoring sessions - Guidance on advanced ROS2 usage</p> <p>Weekly Milestones (Weeks 7\u201316)</p>"},{"location":"initialplan/#weekly-milestones-table-simplified","title":"\ud83d\udcc5 Weekly Milestones Table (Simplified)","text":"Week Hardware Integration Interface Development Controls &amp; Autonomy Assignment Focus / Deliverable Status Week 7 TurtleBot4 bring-up, sensor validation Set up GitHub Pages, basic project website Define system architecture \ud83d\udfe2 Team Assignment 1: Concept, goals, UI mockup \u2705 Complete Week 8 Depth camera, ultrasonic sensor setup RViz mockup, GUI layout draft SLAM stack intro \ud83d\udfe2 TA1 continued: Planning + visuals \u2705 Complete Week 9 Sensor fusion (LiDAR+IMU+Odom) GUI RViz integration Localization tested with fusion \ud83d\udfe2 TA1 final touches, prepare for SLAM \u2705 Complete Week 10 SLAM + map saving setup Real-time plots in GUI SLAM navigation working demo \ud83d\udfe2 Team Assignment 2: SLAM results, depth integration \u2705 Complete Week 11 Costmap layers setup GUI updates: status log, alerts Patrol logic implementation \ud83d\udfe2 TA2 submission; begin full behavior testing \u2705 Complete Week 12 Mode toggle prep (SLAM \u2194 AMCL) Full GUI \u2192 ROS interaction Navigation tuning \ud83d\udfe2 Team Assignment 3: Data filtering, GUI integration \u2705 Complete Week 13 GUI live display (alerts, metrics) GUI control (pause/resume, alerts) Replanning &amp; manual override \ud83d\udfe0 TA3 wrap-up; polish before dry-run \ud83d\udd04 In Progress Week 14 TurtleBot full integration (real test) Export GUI logs, feedback polish SLAM vs AMCL toggle test \ud83d\udfe0 Begin Team Assignment 4: Testing + autonomy \ud83d\udd04 In Progress Week 15 Full autonomy dry run + backups Auto-logging, restart controls Behavior tree + fault handling \ud83d\udd18 TA4 final polish; prepare for live demo \u2b1c Not Started Week 16 In-class demo setup Final GUI build and docs Final validation: autonomy + GUI monitoring \ud83c\udfaf Final deliverables submission + demo \u2b1c Not Started <p>\u2705 Status Key:</p> <ul> <li>\u2705 Complete (Weeks 7\u201312)</li> <li>\ud83d\udd04 In Progress (Weeks 13\u201314)</li> <li>\u2b1c Not Started (Weeks 15\u201316)</li> </ul>"},{"location":"launch/","title":"\ud83d\ude80 Launch Instructions \u2013 <code>warehouse_simulation-main</code>","text":"<p>This guide explains how to bring up the complete warehouse simulation and navigation stack from the <code>warehouse_simulation-main</code> project.</p>"},{"location":"launch/#prerequisites","title":"\ud83d\udce6 Prerequisites","text":"<ul> <li>ROS 2 Humble installed</li> <li><code>ros2_ws</code> workspace created and built</li> <li>The <code>warehouse_simulation-main</code> directory placed in <code>~/ros2_ws/src/</code></li> <li>Workspace built with <code>colcon build</code> and sourced</li> </ul> <pre><code>cd ~/ros2_ws\ncolcon build\nsource install/setup.bash\n</code></pre>"},{"location":"launch/#package-overview","title":"\ud83e\uddf1 Package Overview","text":"Package Description <code>custom_world_pkg</code> Contains the warehouse Gazebo world and models <code>path_planner_pkg</code> Launches the Nav2 stack and sends goals"},{"location":"launch/#step-by-step-launch","title":"\ud83d\udd27 Step-by-Step Launch","text":""},{"location":"launch/#1-launch-gazebo-simulation","title":"1. \ud83c\udfed Launch Gazebo Simulation","text":"<pre><code>ros2 launch custom_world_pkg launch_simulation.launch.py\n</code></pre> <ul> <li>Loads the TurtleBot4 into the custom warehouse Gazebo world</li> <li>Gazebo GUI will open</li> </ul>"},{"location":"launch/#2-launch-navigation-stack","title":"2. \ud83d\uddfa\ufe0f Launch Navigation Stack","text":"<p>Open a new terminal, and run:</p> <pre><code>source ~/ros2_ws/install/setup.bash\nros2 launch path_planner_pkg nav2.launch.py\n</code></pre> <p>This launches: - Map server - AMCL (localization) - Behavior tree navigator - Planner &amp; controller servers</p>"},{"location":"launch/#3-send-goal-from-custom-node","title":"3. \ud83c\udfaf Send Goal from Custom Node","text":"<p>Still in a separate terminal:</p> <pre><code>source ~/ros2_ws/install/setup.bash\nros2 run path_planner mission_executor\n</code></pre> <p>This node: - Sends a goal to <code>/navigate_to_pose</code> - Uses the Nav2 BT Navigator to handle path planning &amp; control</p>"},{"location":"launch/#visualize-in-rviz2","title":"\ud83c\udf9b Visualize in RViz2","text":"<p>Optionally, launch <code>rviz2</code>:</p> <pre><code>rviz2\n</code></pre> <p>Configure to display: - <code>/map</code> - <code>/odom</code> - <code>/path</code> - <code>/tf</code> - <code>/amcl_pose</code> - <code>/goal_pose</code></p>"},{"location":"launch/#final-flow-summary","title":"\ud83e\udde0 Final Flow Summary","text":"<pre><code>Gazebo \u27f6 TF, scan, joint_states\n \u21b3 Nav2 Stack \u27f6 planner, controller, AMCL\n \u21b3 mission_executor.py \u27f6 sends custom goal\n \u21b3 RViz2 \u27f6 real-time feedback and map visualization\n</code></pre>"},{"location":"launch/#output","title":"\u2705 Output","text":"<p>Once launched: - Robot will localize and navigate - Paths will be visible in RViz2 - Simulation runs in Gazebo using your custom world</p>"},{"location":"outputs/","title":"Project Outputs: Autonomous Warehouse Patrolling Robot","text":"<p>This page consolidates the key visual results and architectural outputs from the <code>warehouse_simulation-main</code> project developed by Team 08.</p>"},{"location":"outputs/#final-ros2-architecture-diagram","title":"Final ROS2 Architecture Diagram","text":"<p>A simulated rqt_graph-style layout based on all nodes, topics, and services active during robot navigation.</p> <p></p>"},{"location":"outputs/#rqt_graph-snapshot","title":"rqt_graph Snapshot","text":"<p>This diagram simulates a snapshot of the live ROS2 communication graph generated using <code>rqt_graph</code>.</p> <p></p>"},{"location":"outputs/#navigation-accuracy-patrol-zone-coverage-graph","title":"Navigation Accuracy \u2013 Patrol Zone Coverage Graph","text":"<p>Shows how many patrol zones were successfully visited compared to the expected mission plan.</p> <p></p>"},{"location":"outputs/#anomaly-detection","title":"Anomaly Detection","text":"<p>Illustrates the detection performance across different anomaly types using simulated ground truth.</p> <p></p>"},{"location":"outputs/#demonstration-videos","title":"Demonstration Videos","text":""},{"location":"outputs/#autonomous-patrol-in-action","title":"Autonomous Patrol in Action","text":""},{"location":"outputs/#elevator-pitch","title":"Elevator Pitch","text":""},{"location":"outputs/#simulation-screenshots","title":"Simulation Screenshots","text":""},{"location":"outputs/#web-application-screenshot","title":"Web-Application Screenshot","text":""},{"location":"outputs/#summary","title":"Summary","text":"<p>These outputs validate the core goals of our project: - Real-time autonomous patrolling in a simulated warehouse - Multi-layered ROS2 control stack with Nav2 - Visualized SLAM, anomaly detection, and path planning</p>"},{"location":"ros2_architecture/","title":"Full ROS 2 Architecture \u2013 <code>warehouse_simulation-main</code>","text":""},{"location":"ros2_architecture/#packages","title":"Packages:","text":"<ul> <li><code>custom_world_pkg</code> \u2192 launches Gazebo with warehouse models</li> <li><code>path_planner_pkg</code> \u2192 handles Nav2 stack and goal navigation</li> </ul>"},{"location":"ros2_architecture/#nodes-identified","title":"Nodes Identified","text":"Node Name Source Description <code>/gazebo</code> Gazebo Simulator Physics &amp; visualization engine <code>/robot_state_publisher</code> Auto-started in Gazebo Publishes joint TF from URDF <code>/mission_executor</code> <code>mission_executor.py</code> in <code>path_planner_pkg</code> Sends goals to <code>/navigate_to_pose</code> <code>/map_server</code> Nav2 (launched via <code>planner.launch.py</code>) Publishes static map <code>/amcl</code> Nav2 localization Computes robot pose on map <code>/planner_server</code> Nav2 plugin Plans global path <code>/controller_server</code> Nav2 plugin Executes local path <code>/nav2_bt_navigator</code> Nav2 behavior tree Orchestrates full navigation <code>/rviz2</code> Visualization GUI for maps, robot, goals"},{"location":"ros2_architecture/#topics-flow","title":"Topics Flow","text":"Topic From \u2192 To Notes <code>/navigate_to_pose/_action/goal</code> <code>mission_executor</code> \u2192 <code>nav2_bt_navigator</code> Sends goal coordinates <code>/map</code> <code>map_server</code> \u2192 <code>amcl</code>, <code>rviz2</code> Occupancy map for localization and display <code>/scan</code> <code>gazebo</code> (LiDAR plugin) \u2192 <code>amcl</code> Simulated 2D LiDAR <code>/amcl_pose</code> <code>amcl</code> \u2192 <code>nav2_bt_navigator</code> Robot pose estimate <code>/plan_request</code> <code>nav2_bt_navigator</code> \u2192 <code>planner_server</code> Global path request <code>/path</code> <code>planner_server</code> \u2192 <code>controller_server</code> Path handoff <code>/cmd_vel</code> <code>controller_server</code> \u2192 <code>gazebo</code> Robot wheel commands <code>/tf</code> <code>robot_state_publisher</code> \u2192 <code>rviz2</code> Robot transforms <code>/feedback</code> <code>nav2_bt_navigator</code> \u2192 <code>rviz2</code> Goal feedback <code>/joint_states</code> <code>gazebo</code> \u2192 <code>robot_state_publisher</code> Joint info for TF"},{"location":"ros2_architecture/#how-the-system-flows","title":"How the System Flows","text":"<pre><code>[mission_executor.py]\n     |\n     v\n[navigate_to_pose] --[BT]--&gt; [planner_server] --&gt; [controller_server] --&gt; [cmd_vel] --&gt; [gazebo]\n\n[amcl] &lt;-- [scan] + [map] &lt;-- [map_server]\n    |\n   [amcl_pose] --&gt; [nav2_bt_navigator]\n\n[tf], [map], [feedback] --&gt; [rviz2]\n</code></pre>"},{"location":"ros2_architecture/#diagram-visual-ros2-graph","title":"Diagram: Visual ROS2 Graph","text":"<p>\ud83d\udcf7 </p>"},{"location":"ros2_architecture/#summary","title":"Summary","text":"<p>The simulation launches: - A robot inside a Gazebo warehouse - Sends goals from a custom node (<code>mission_executor</code>) - Uses the standard Nav2 stack to plan and navigate autonomously - RViz2 for visualization and debugging</p>"},{"location":"videos/","title":"\ud83c\udfa5 Project Demo Videos","text":""},{"location":"videos/#autonomous-patrol-in-action","title":"\ud83d\udce6 Autonomous Patrol in Action","text":"<p>This video demonstrates the TurtleBot4 autonomously navigating through a custom warehouse simulation built in Gazebo. The robot performs a full patrol loop using Nav2, reacts to obstacles, and follows predefined waypoints. You\u2019ll see: - Live SLAM-based navigation - Dynamic re-planning when paths are blocked - Accurate localization  - Integration of sensor data for robust movement</p>"},{"location":"videos/#elevator-pitch","title":"\ud83c\udfa4 Elevator Pitch","text":"<p>This 60-second pitch summarizes the motivation, goals, and highlights of our project: the Autonomous Warehouse Patrolling Robot.</p>"},{"location":"videos/#gui-real-time-sensor-data-live-demo-progress","title":"\ud83d\udda5\ufe0f GUI Real-Time Sensor Data (Live Demo Progress)","text":"<p>This video showcases the real-time graphical user interface (GUI) developed to monitor and control the robot\u2019s patrol behavior. The GUI includes: - LiDAR scan visualization - Current patrol zone mapping - Anomaly flagging and logging - Status updates: battery level, distance, alerts</p>"}]}